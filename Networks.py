# -*- coding: utf-8 -*-
"""A lot of Networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e3n0CSgUOeA7eJrZC26GOMY2SNJFwKZH
"""

import pandas as pd
import random
import numpy as np
import matplotlib.pyplot as plt
from torchvision import transforms, utils
import torch
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
import torch.nn as nn
import torch.nn.functional as F
from google.colab import drive
import math
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error

drive.mount('/content/drive')


class Dataset(Dataset):

    def __init__(self, csv_file, transform=None):

        self.data = pd.read_csv(csv_file, index_col=0, header=0).to_numpy()
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):

        if torch.is_tensor(idx):
            idx = idx.tolist()

        sample = self.data[idx]
        features = np.array(sample[2:], dtype=np.float32)
        labels = np.array([sample[0], sample[1]], dtype=np.float32)  # sample[0] - 1

        if self.transform is not None:
            sample = self.transform(theta_array=torch.linspace(10, 65, len(features)), data_arr=features, noise=True,
                                    snr=3)  #

        return features, labels


# ɵ

def Mal_func(theta_array, data_arr, noise=False, snr=0):
    vector_length = len(theta_array)
    w_array = np.zeros(vector_length)
    for i in range(vector_length):
        theta = theta_array[i]
        w = 1 / theta * math.exp(-2 * (math.log(theta / 54, math.e)) ** 2)
        w_array[i] = w
        data_arr[i] = data_arr[i] * w_array[i]

    if noise == True:
        min_val = min(data_arr)
        for i in range(len(data_arr)):
            data_arr[i] = data_arr[i] + (2 * np.random.random_sample() - 1) * min_val / snr
    return data_arr


train_data = Dataset(csv_file='/content/drive/MyDrive/Scattering_Group/PyMieScatt/scattering_dataset/data.csv',
                     transform=Mal_func)

# test_data = Dataset(csv_file='/content/drive/MyDrive/Scattering_Group/PyMieScatt/latex_database_100random_d_3.5_3.79_n_1.56_1.589.csv',
#                                            transform = Mal_func )
test_data = Dataset(csv_file='/content/drive/MyDrive/Scattering_Group/PyMieScatt/scattering_dataset/test_data.csv',
                    transform=Mal_func)

"""проверка функции мальцева"""

# data = pd.read_csv('/content/drive/MyDrive/Scattering_Group/PyMieScatt/scattering_dataset/data.csv',  index_col=0 , header=0).to_numpy()
# plt.plot(np.linspace(10,65,256), data[1500][2:])

# plt.plot( np.linspace(10,65,256), train_data[1500][0])
# plt.xlabel('Полярный угол, ɵ', fontsize = 13)
# plt.ylabel('Интенсивность', fontsize = 13)
# plt.title('Преобразованная индикатриса SNR = 3', fontsize = 13)
# plt.savefig('Преобразованная индикатриса SNR = 3.svg')

batch_size = 50
data_size = len(train_data)
# validation_fraction = .2

# val_split = int(np.floor((validation_fraction) * data_size))
indices = list(range(data_size))
np.random.seed(42)
np.random.shuffle(indices)

# val_indices, train_indices = indices[:val_split], indices[val_split:]
train_indices = indices
train_sampler = SubsetRandomSampler(train_indices)
# val_sampler = SubsetRandomSampler(val_indices)

train_loader = DataLoader(train_data, batch_size=batch_size,
                          sampler=train_sampler)

# val_loader = DataLoader(train_data, batch_size=batch_size,
#                                            sampler=val_sampler)

test_loader = DataLoader(test_data, batch_size=len(test_data))


def graphs(loss_history, metrics_history, prediction_arr, labels_arr, epochs):
    plt.figure(figsize=(12, 12))
    plt.subplot(321)
    plt.plot(epochs, loss_history, label='loss')
    plt.xlabel('Номер эпохи')
    plt.ylabel('Функция потери')
    plt.yscale('log')
    plt.legend()

    plt.subplot(323)
    plt.plot(epochs, metrics_history, label='metrics = mape ')
    # plt.plot(epochs, val_history,label = 'val_metrics = mape' )
    plt.yscale('log')
    plt.legend()

    plt.subplot(324)
    plt.plot(prediction_arr[:, 0], prediction_arr[:, 1], linewidth=0, marker='o', label='pred')
    plt.plot(labels_arr[:, 0], labels_arr[:, 1], linewidth=0, marker='o', label='true')

    plt.xlabel('Относительный показатель преломления m')
    plt.ylabel('Дифракционный параметр x')
    plt.legend()

    plt.subplot(325)
    m_true = labels_arr[:, 0]
    m_pred = prediction_arr[:, 0]
    m_err = abs(m_true[:] - m_pred[:])
    plt.hist(m_err, bins=50)

    length = len(m_err)
    text_1 = "%s" % float('%.2g' % np.median(m_err[(length - 11):(length - 1)]))
    print('Абсолютная ошибка m = ' + text_1)
    plt.xlabel('Ошибка по m')
    plt.ylabel('Число частиц')

    plt.subplot(326)
    x_true = labels_arr[:, 1]
    x_pred = prediction_arr[:, 1]
    x_err = abs(x_true - x_pred) / x_true * 100
    plt.hist(x_err, bins=50)

    length = len(x_err)
    text_1 = "%s" % float('%.2g' % np.median(x_err[(length - 11):(length - 1)]))  # '%s' % float('%.1g' % 0.012)
    print('mape_x = ' + text_1 + '%')
    plt.xlabel('Ошибка по x')
    plt.ylabel('Число частиц')

    plt.show()


def metrics_mape(prediction, labels):
    return np.mean(abs((prediction - labels) / labels) * 100)


def loss_mape(prediction, labels):
    return torch.mean(abs(prediction - labels) / labels * 100)


def loss2(prediction, labels):
    range_x = max(labels[:, 1]) - min(labels[:, 1])
    loss_x = abs(prediction[:, 1] - labels[:, 1]) / (range_x)
    range_m = max(labels[:, 0]) - min(labels[:, 0])
    loss_m = abs(prediction[:, 0] - labels[:, 0]) / (range_m)
    return torch.mean(loss_x + loss_m) / 2


def val_epoch(model, val_loader, metrics):
    model.eval()
    with torch.no_grad():
        metrics_accum = 0
        for i_step, (x, y) in enumerate(val_loader):
            x = x.unsqueeze(1)
            val_prediction = model(x.to(device))
            metrics_value = metrics(val_prediction.cpu().detach().numpy(), y.detach().numpy())
            metrics_accum += metrics_value
    model.train()
    return metrics_accum / i_step  # i_step = batch_size


def train_epoch(model, train_loader, loss, metrics):
    metrics_accum = 0
    loss_accum = 0
    prediction_arr = []
    labels_arr = []
    for i_step, (x, y) in enumerate(train_loader):
        x = x.unsqueeze(1)
        prediction = model(x.to(device)).squeeze(1)

        loss_value = loss((prediction / y.to(device)), torch.ones(prediction.shape).to(device))

        metrics_value = metrics(prediction.cpu().detach().numpy(), y.detach().numpy())

        optimizer.zero_grad()
        loss_value.backward()
        optimizer.step()

        metrics_accum += metrics_value
        loss_accum += loss_value.item()

        prediction_arr.append(prediction.cpu().detach().numpy())
        labels_arr.append(y.detach().numpy())

    ave_loss = loss_accum / i_step
    ave_metrics = metrics_accum / i_step

    return ave_loss, ave_metrics, prediction_arr, labels_arr


class Loss2(nn.Module):
    def __call__(self, *args, **kwargs):
        return loss2(*args, **kwargs)


class Mean_Abs_Pepcentage_Error(nn.Module):
    def __call__(self, *args, **kwargs):
        return loss_mape(*args, **kwargs)


def ri_train_epoch(model, train_loader, loss, metrics):
    metrics_accum = 0
    loss_accum = 0
    prediction_arr = []
    labels_arr = []
    for i_step, (x, y) in enumerate(train_loader):
        x = x.unsqueeze(1)

        prediction = model(x.to(device)).squeeze(1)
        y = y[:, 0]
        loss_value = loss((prediction / y.to(device)), torch.ones(prediction.shape).to(device))

        metrics_value = metrics(prediction.cpu().detach().numpy(), y.detach().numpy())

        optimizer.zero_grad()
        loss_value.backward()
        optimizer.step()

        metrics_accum += metrics_value
        loss_accum += loss_value.item()

        prediction_arr.append(prediction.cpu().detach().numpy())
        labels_arr.append(y.detach().numpy())

    ave_loss = loss_accum / i_step
    ave_metrics = metrics_accum / i_step

    return ave_loss, ave_metrics, prediction_arr, labels_arr


if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')
print(device)


def train(num_epochs, loss, metrics, scheduler, val_loader, train_loader):
    loss_history = []
    train_history = []
    val_history = []
    epochs = np.linspace(0, num_epochs, num_epochs)
    for epoch in range(num_epochs):
        model.train()
        ave_loss, ave_metrics, prediction, labels = train_epoch(model, train_loader, loss, metrics)
        scheduler.step()
        print(optimizer.param_groups[0]["lr"])

        # val_history_value = val_epoch(model, val_loader, metrics)

        # val_history.append(val_history_value)
        loss_history.append(float(ave_loss))
        train_history.append(float(ave_metrics))

        print("Epoch: %d, Average loss: %f, average metric (mape): %f" % (
        (epoch + 1), ave_loss, ave_metrics))  # , val metric (mape): %f , val_history_value))

    return [loss_history, train_history, val_history, prediction, labels, epochs]


def ri_train(num_epochs, loss, metrics, scheduler, val_loader, train_loader):
    loss_history = []
    train_history = []
    val_history = []
    epochs = np.linspace(0, num_epochs, num_epochs)
    for epoch in range(num_epochs):
        model.train()
        ave_loss, ave_metrics, prediction, labels = ri_train_epoch(model, train_loader, loss, metrics)
        scheduler.step()
        print(optimizer.param_groups[0]["lr"])

        # val_history_value = val_epoch(model, val_loader, metrics)

        # val_history.append(val_history_value)
        loss_history.append(float(ave_loss))
        train_history.append(float(ave_metrics))

        print("Epoch: %d, Average loss: %f, average metric (mape): %f" % (
        (epoch + 1), ave_loss, ave_metrics))  # , val metric (mape): %f , val_history_value))

    return [loss_history, train_history, val_history, prediction, labels, epochs]


def ri_test(metrics):
    model.eval()
    with torch.no_grad():
        for i_step, (x, y) in enumerate(test_loader):
            x = x.unsqueeze(1)
            prediction = model(x.to(device)).squeeze(1)
            y = y[:, 0]
            metrics_value = metrics(prediction.cpu().detach().numpy(), y.detach().numpy())
            prediction_arr = (prediction.cpu().detach().numpy())
            labels_arr = (y.cpu().detach().numpy())

    labels_arr = np.squeeze(labels_arr)
    prediction_arr = np.squeeze(prediction_arr)

    m_true = labels_arr
    m_pred = prediction_arr
    print('MAPE m-1 (тест) = ', mean_absolute_percentage_error(m_true - 1, m_pred - 1) * 100)
    print('mae m ', mean_absolute_error(m_true, m_pred))
    # x_true = labels_arr[:, 1]
    # x_pred = prediction_arr[:,1]
    # print('MAPE ошибка x (тест) = ' , mean_absolute_percentage_error(x_true, x_pred)*100)

    # print("Ошибка (тест): " , mean_absolute_percentage_error(labels_arr, prediction_arr)*100)#*2)
    return prediction_arr, labels_arr


def test(metrics):
    model.eval()
    with torch.no_grad():
        for i_step, (x, y) in enumerate(test_loader):
            x = x.unsqueeze(1)
            prediction = model(x.to(device)).squeeze(1)

            metrics_value = metrics(prediction.cpu().detach().numpy(), y.detach().numpy())
            prediction_arr = (prediction.cpu().detach().numpy())
            labels_arr = (y.cpu().detach().numpy())

    labels_arr = np.squeeze(labels_arr)
    prediction_arr = np.squeeze(prediction_arr)

    m_true = labels_arr[:, 0]
    m_pred = prediction_arr[:, 0]
    print('MAPE m-1 (тест) = ', mean_absolute_percentage_error(m_true - 1, m_pred - 1) * 100)
    print('mae m ', mean_absolute_error(m_true, m_pred))
    x_true = labels_arr[:, 1]
    x_pred = prediction_arr[:, 1]
    print('MAPE ошибка x (тест) = ', mean_absolute_percentage_error(x_true, x_pred) * 100)

    print("Ошибка (тест): ",
          mean_absolute_percentage_error(y.detach().numpy(), prediction.cpu().detach().numpy()) * 100 * 2)
    return prediction_arr, labels_arr


class Modified_seven_ConvNet_Tanh(nn.Module):
    def __init__(self):
        super(Modified_seven_ConvNet_Tanh, self).__init__()
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=2)
        self.bn2 = nn.BatchNorm1d(num_features=2)
        self.conv2 = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm1d(num_features=4)
        self.conv3 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=7, padding=2)
        self.bn4 = nn.BatchNorm1d(num_features=8)
        self.conv4 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=11, padding=2)
        self.bn5 = nn.BatchNorm1d(num_features=16)
        self.conv5 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=13, padding=2)
        self.bn6 = nn.BatchNorm1d(num_features=32)
        self.conv6 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=17, padding=2)
        self.bn7 = nn.BatchNorm1d(num_features=16)
        self.conv7 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=23, padding=2)
        self.bn8 = nn.BatchNorm1d(num_features=8)
        self.fc1 = nn.Linear(1696, 180)  #
        self.fc2 = nn.Linear(180, 2)

    def forward(self, x):
        x = self.bn1(x)
        x = torch.tanh(self.conv1(x))
        x = self.bn2(x)
        x = torch.tanh(self.conv2(x))
        x = self.bn3(x)
        x = torch.tanh(self.conv3(x))
        x = self.bn4(x)
        x = torch.tanh(self.conv4(x))
        x = self.bn5(x)
        x = torch.tanh(self.conv5(x))
        x = self.bn6(x)
        x = torch.tanh(self.conv6(x))
        x = self.bn7(x)
        x = torch.tanh(self.conv7(x))
        x = self.bn8(x)

        x = torch.flatten(x, 1)
        x = torch.tanh(self.fc1(x))
        x = F.relu(self.fc2(x))
        return x


# 326064 params

class FC_Net_2(nn.Module):
    def __init__(self):
        super(FC_Net_2, self).__init__()

        self.fc1 = nn.Linear(256, 256)
        self.fc2 = nn.Linear(256, 512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 128)
        self.fc5 = nn.Linear(128, 64)
        self.fc6 = nn.Linear(64, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x))
        x = F.relu(self.fc6(x))

        return x


# 304194 params

class FC_Net(nn.Module):
    def __init__(self):
        super(FC_Net, self).__init__()

        self.fc1 = nn.Linear(256, 256)
        self.fc2 = nn.Linear(256, 300)
        self.fc3 = nn.Linear(300, 256)
        self.fc4 = nn.Linear(256, 128)
        self.fc5 = nn.Linear(128, 64)
        self.fc6 = nn.Linear(64, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x))
        x = F.relu(self.fc6(x))

        return x


# 261230 params

class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=21, kernel_size=7, padding=3)
        # Lin = Lout => p = (k_s-1)/2
        # Lout = Lin
        self.conv2 = nn.Conv1d(in_channels=21, out_channels=21, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(in_channels=21, out_channels=21, kernel_size=3, padding=1)
        self.conv4 = nn.Conv1d(in_channels=21, out_channels=21, kernel_size=3, padding=1)

        # Lout = Lin/2 and channels_out*2 (due to 34-layers plains and 34-layers ResNet)  => p = (k-1)/2 - Lin/4
        self.conv5 = nn.Conv1d(in_channels=21, out_channels=42, kernel_size=3, stride=2, padding=0)
        # Lout = Lin
        self.conv6 = nn.Conv1d(in_channels=42, out_channels=42, kernel_size=3, padding=1)
        self.conv7 = nn.Conv1d(in_channels=42, out_channels=42, kernel_size=3, padding=1)

        # Lout = Lin/2 and channels_out*2
        self.conv8 = nn.Conv1d(in_channels=42, out_channels=84, kernel_size=3, stride=2, padding=0)
        # Lout = Lin
        self.conv9 = nn.Conv1d(in_channels=84, out_channels=84, kernel_size=3, padding=1)
        self.conv10 = nn.Conv1d(in_channels=84, out_channels=84, kernel_size=3, padding=1)

        # reducing out_ch value. Lout = Lin
        self.conv11 = nn.Conv1d(in_channels=84, out_channels=42, kernel_size=1, padding=0)
        self.conv12 = nn.Conv1d(in_channels=42, out_channels=21, kernel_size=1, padding=0)
        self.conv13 = nn.Conv1d(in_channels=21, out_channels=1, kernel_size=1, padding=0)

        self.fc1 = nn.Linear(63, 2)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = F.relu(self.conv5(x))
        x = F.relu(self.conv6(x))
        x = F.relu(self.conv7(x))
        x = F.relu(self.conv8(x))
        x = F.relu(self.conv9(x))
        x = F.relu(self.conv10(x))
        x = F.relu(self.conv11(x))
        x = F.relu(self.conv12(x))
        x = F.relu(self.conv13(x))

        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))

        return x


# 75353 params

class Modified_seven_ConvNet(nn.Module):
    def __init__(self):
        super(Modified_seven_ConvNet, self).__init__()
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=2)
        self.bn2 = nn.BatchNorm1d(num_features=2)
        self.conv2 = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm1d(num_features=4)
        self.conv3 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=7, padding=2)
        self.bn4 = nn.BatchNorm1d(num_features=8)
        self.conv4 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=11, padding=2)
        self.bn5 = nn.BatchNorm1d(num_features=16)
        self.conv5 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=13, padding=2)
        self.bn6 = nn.BatchNorm1d(num_features=32)
        self.conv6 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=17, padding=2)
        self.bn7 = nn.BatchNorm1d(num_features=16)
        self.conv7 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=23, padding=2)
        self.bn8 = nn.BatchNorm1d(num_features=8)
        self.fc1 = nn.Linear(1696, 180)  #
        self.fc2 = nn.Linear(180, 2)

    def forward(self, x):
        x = self.bn1(x)
        x = F.relu(self.conv1(x))
        x = self.bn2(x)
        x = F.relu(self.conv2(x))
        x = self.bn3(x)
        x = F.relu(self.conv3(x))
        x = self.bn4(x)
        x = F.relu(self.conv4(x))
        x = self.bn5(x)
        x = F.relu(self.conv5(x))
        x = self.bn6(x)
        x = F.relu(self.conv6(x))
        x = self.bn7(x)
        x = F.relu(self.conv7(x))
        x = self.bn8(x)

        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return x


# 326064 params

class RI_output_Modified_seven_ConvNet(nn.Module):
    def __init__(self):
        super(RI_output_Modified_seven_ConvNet, self).__init__()
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=2)
        self.bn2 = nn.BatchNorm1d(num_features=2)
        self.conv2 = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm1d(num_features=4)
        self.conv3 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=7, padding=2)
        self.bn4 = nn.BatchNorm1d(num_features=8)
        self.conv4 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=11, padding=2)
        self.bn5 = nn.BatchNorm1d(num_features=16)
        self.conv5 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=13, padding=2)
        self.bn6 = nn.BatchNorm1d(num_features=32)
        self.conv6 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=17, padding=2)
        self.bn7 = nn.BatchNorm1d(num_features=16)
        self.conv7 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=23, padding=2)
        self.bn8 = nn.BatchNorm1d(num_features=8)
        self.fc1 = nn.Linear(1696, 180)  #
        self.fc2 = nn.Linear(180, 1)

    def forward(self, x):
        x = self.bn1(x)
        x = F.relu(self.conv1(x))
        x = self.bn2(x)
        x = F.relu(self.conv2(x))
        x = self.bn3(x)
        x = F.relu(self.conv3(x))
        x = self.bn4(x)
        x = F.relu(self.conv4(x))
        x = self.bn5(x)
        x = F.relu(self.conv5(x))
        x = self.bn6(x)
        x = F.relu(self.conv6(x))
        x = self.bn7(x)
        x = F.relu(self.conv7(x))
        x = self.bn8(x)

        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return x


# 326064 params

class StagedClusterNet(nn.Module):
    def __init__(self):
        super(StagedClusterNet, self).__init__()
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv11 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=2)
        self.bn2 = nn.BatchNorm1d(num_features=2)
        self.conv12 = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm1d(num_features=4)
        self.conv13 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=7, padding=2)
        self.bn4 = nn.BatchNorm1d(num_features=8)
        self.conv14 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=11, padding=2)
        self.bn5 = nn.BatchNorm1d(num_features=16)
        self.conv15 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=13, padding=2)
        self.bn6 = nn.BatchNorm1d(num_features=32)
        self.conv16 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=17, padding=2)
        self.bn7 = nn.BatchNorm1d(num_features=16)
        self.conv17 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=23, padding=2)
        self.bn8 = nn.BatchNorm1d(num_features=8)
        self.fc11 = nn.Linear(1696, 180)  #
        self.fc12 = nn.Linear(180, 2)  # (m,x)

        self.conv21 = nn.Conv1d(in_channels=1, out_channels=21, kernel_size=7, padding=3)
        # Lin = Lout => p = (k_s-1)/2
        # Lout = Lin
        self.conv22 = nn.Conv1d(in_channels=21, out_channels=21, kernel_size=3, padding=1)
        self.conv23 = nn.Conv1d(in_channels=21, out_channels=21, kernel_size=3, padding=1)
        self.conv24 = nn.Conv1d(in_channels=21, out_channels=21, kernel_size=3, padding=1)

        # Lout = Lin/2 and channels_out*2 (due to 34-layers plains and 34-layers ResNet)  => p = (k-1)/2 - Lin/4
        self.conv25 = nn.Conv1d(in_channels=21, out_channels=42, kernel_size=3, stride=2, padding=0)
        # Lout = Lin
        self.conv26 = nn.Conv1d(in_channels=42, out_channels=42, kernel_size=3, padding=1)
        self.conv27 = nn.Conv1d(in_channels=42, out_channels=42, kernel_size=3, padding=1)

        # Lout = Lin/2 and channels_out*2
        self.conv28 = nn.Conv1d(in_channels=42, out_channels=84, kernel_size=3, stride=2, padding=0)
        # Lout = Lin
        self.conv29 = nn.Conv1d(in_channels=84, out_channels=84, kernel_size=3, padding=1)
        self.conv210 = nn.Conv1d(in_channels=84, out_channels=84, kernel_size=3, padding=1)

        # reducing out_ch value. Lout = Lin
        self.conv211 = nn.Conv1d(in_channels=84, out_channels=42, kernel_size=1, padding=0)
        self.conv212 = nn.Conv1d(in_channels=42, out_channels=21, kernel_size=1, padding=0)
        self.conv213 = nn.Conv1d(in_channels=21, out_channels=1, kernel_size=1, padding=0)

        self.fc21 = nn.Linear(64, 2)  # 63+1

    def forward(self, inp):
        x = self.bn1(inp)
        x = F.relu(self.conv11(x))
        x = self.bn2(x)
        x = F.relu(self.conv12(x))
        x = self.bn3(x)
        x = F.relu(self.conv13(x))
        x = self.bn4(x)
        x = F.relu(self.conv14(x))
        x = self.bn5(x)
        x = F.relu(self.conv15(x))
        x = self.bn6(x)
        x = F.relu(self.conv16(x))
        x = self.bn7(x)
        x = F.relu(self.conv17(x))
        x = self.bn8(x)

        x = torch.flatten(x, 1)
        x = F.relu(self.fc11(x))
        x = F.relu(self.fc12(x))  # (m,x)

        x1 = torch.reshape(x[:, 0], (len(x[:, 0]), 1))  # (m)

        x = F.relu(self.conv21(inp))
        x = F.relu(self.conv22(x))
        x = F.relu(self.conv23(x))
        x = F.relu(self.conv24(x))
        x = F.relu(self.conv25(x))
        x = F.relu(self.conv26(x))
        x = F.relu(self.conv27(x))
        x = F.relu(self.conv28(x))
        x = F.relu(self.conv29(x))
        x = F.relu(self.conv210(x))
        x = F.relu(self.conv211(x))
        x = F.relu(self.conv212(x))
        x = F.relu(self.conv213(x))

        x = torch.flatten(x, 1)
        x2 = torch.cat([x, x1], dim=1)

        x = F.relu(self.fc21(x2))

        return x


# 401417

model = RI_output_Modified_seven_ConvNet().to(device)
optimizer = torch.optim.Adam(model.parameters())
# criterion = Mean_Abs_Pepcentage_Error()
criterion = nn.MSELoss()
# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, 1)

pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
pytorch_total_params

num_epochs = 1
val_loader = 0  # временно
history = ri_train(num_epochs=num_epochs, loss=criterion, metrics=metrics_mape, scheduler=scheduler,
                   val_loader=val_loader, train_loader=train_loader)
loss_history, train_history, val_history, epochs = history[0], history[1], history[2], history[5]

# PATH = "/content/drive/MyDrive/Scattering_Group/PyMieScatt/model_FC_Net_loss_mse__data_80k_CosineAnnealingLR_noise_with_validation.pt"

# torch.save({
#             'epoch': epochs,
#             'model_state_dict': model.state_dict(),
#             'optimizer_state_dict': optimizer.state_dict(),
#             'loss': criterion,
#             'loss_history':loss_history,
#             'train_history': train_history,
#             'val_history': val_history
#             }, PATH)

prediction_arr, labels_arr = ri_test(metrics=metrics_mape)  # history[3], history[4] #
graphs(loss_history, train_history, val_history, prediction_arr, labels_arr, epochs)

model = ConvNet().to(device)
val_loader = 0  # временно
PATH = '/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/batch0.05k_traindata100k_ConvNet_nosc (2).pt'
# PATH = '/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/batch0.05k_traindata100k_Modified_seven_ConvNet_nosc_restart.pt'#model = nn.DataParallel(model)

optimizer = torch.optim.Adam(model.parameters())
checkpoint = torch.load(PATH)  # , map_location=torch.device('cpu'))
model.load_state_dict(checkpoint['model_state_dict'], strict=False)  #
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epochs = checkpoint['epoch']
loss = checkpoint['loss']

loss_history = checkpoint['loss_history']
train_history = checkpoint['train_history']
# val_history = checkpoint['val_history']

# model.train()

num_epochs = 5
# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2, 1)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)

# history = train(num_epochs = num_epochs, loss = loss, metrics = metrics_mape, scheduler = scheduler, val_loader = val_loader, train_loader =train_loader )
# loss_history,train_history,  val_history = np.append(loss_history, history[0]), np.append(train_history, history[1]), np.append(val_history, history[2])
epochs = np.linspace(0, len(loss_history), len(loss_history))

# PATH = "/content/drive/MyDrive/Scattering_Group/PyMieScatt/model_FC_Net_loss_mse__data_80k_CosineAnnealingLR_noise_with_validation.pt"
# torch.save({
#             'epoch': epochs,
#             'model_state_dict': model.state_dict(),
#             'optimizer_state_dict': optimizer.state_dict(),
#             'loss': loss,
#             'loss_history': loss_history,
#             'train_history': train_history,
#             'val_history': val_history
#             }, PATH)

prediction_arr, labels_arr = test(metrics=metrics_mape)  # history[3], history[4] #
# graphs(loss_history, train_history, val_history, prediction_arr, labels_arr, epochs)

m_true = labels_arr[:, 0]
m_pred = prediction_arr[:, 0]
m_err = abs(m_true - m_pred)
plt.hist(m_err, bins=50)

plt.xlabel('Абсолютная ошибка m')
plt.ylabel('Число частиц')
plt.savefig('Абсолютная ошибка m.svg')

x_true = labels_arr[:, 1]
x_pred = prediction_arr[:, 1]
x_err = abs(x_true - x_pred) / x_true * 100
plt.hist(x_err, bins=50)

plt.xlabel('Относительная ошибка x, %')
plt.ylabel('Число частиц')
plt.savefig('Относительная ошибка x, %.svg')

plt.plot(epochs, loss_history)
plt.xlabel('Номер эпохи')
plt.ylabel('Функция потери')
plt.yscale('log')

plt.plot(epochs, train_history)

# plt.plot(epochs, val_history,label = 'val_metrics = mape' )
plt.xlabel('Номер эпохи')
plt.ylabel('Метрика')
plt.yscale('log')


# TEST#

def graphs_1(prediction_arr, labels_arr):
    # plt.figure(figsize=(12,12))

    plt.plot(prediction_arr[:, 0], prediction_arr[:, 1], linewidth=0, marker='o', label='Предсказнные значения')
    plt.plot(labels_arr[:, 0], labels_arr[:, 1], linewidth=0, marker='o', label='Истинные значения')

    plt.title('SNR = 5')
    plt.xlabel('Относительный показатель преломления m')
    plt.ylabel('Дифракционный параметр x')
    plt.legend()

    plt.show()


model = Modified_seven_ConvNet().to(device)
PATH = "/content/drive/MyDrive/Scattering_Group/PyMieScatt/model_Modified_seven_ConvNet_loss_mse__data_100k_scheduler_CosineAnnealingLR_noise_without_validation.pt"
optimizer = torch.optim.Adam(model.parameters())

checkpoint = torch.load(PATH, map_location=torch.device('cpu'))
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

epochs = checkpoint['epoch']
loss = checkpoint['loss']
loss_history = checkpoint['loss_history']
train_history = checkpoint['train_history']
val_history = checkpoint['val_history']

epochs = np.linspace(0, len(loss_history), len(loss_history))

prediction_arr, labels_arr = test(metrics=metrics_mape)  # history[3], history[4] #
graphs(loss_history, train_history, val_history, prediction_arr, labels_arr, epochs)

# pip install PyMieScatt

import PyMieScatt as ps

sample = prediction_arr[10]
r_i = sample[0] * 1.333
d = sample[1] * 660 / (np.pi * 1.333)
_, _, _, ind_nn = ps.ScatteringFunction(m=r_i,
                                        wavelength=660,
                                        diameter=d,
                                        nMedium=1.333,
                                        minAngle=10,
                                        maxAngle=65,
                                        angularResolution=55 / 255,
                                        space='theta',
                                        angleMeasure='degrees')
plt.plot(np.linspace(10, 65, 256), ind_nn, label='Предсказанная индикатриса')
plt.title('SNR = 5')
plt.legend()
sample = labels_arr[10]
r_i = sample[0] * 1.333
d = sample[1] * 660 / (np.pi * 1.333)
_, _, _, ind_mie = ps.ScatteringFunction(m=r_i,
                                         wavelength=660,
                                         diameter=d,
                                         nMedium=1.333,
                                         minAngle=10,
                                         maxAngle=65,
                                         angularResolution=55 / 255,
                                         space='theta',
                                         angleMeasure='degrees')
plt.plot(np.linspace(10, 65, 256), ind_mie, label='Истинная индикатриса')

plt.xlabel('Азимутальный угол, ɵ')
plt.ylabel("Интенсивность")
plt.legend()
r = 0
for i in range(256):
    r += (ind_nn[i] - ind_mie[i]) ** 2
r / 256

g = 0
for i in range(len(prediction_arr)):
    sample = prediction_arr[i]
    r_i = sample[0] * 1.333
    d = sample[1] * 660 / (np.pi * 1.333)
    _, _, _, ind_nn = ps.ScatteringFunction(m=r_i,
                                            wavelength=660,
                                            diameter=d,
                                            nMedium=1.333,
                                            minAngle=10,
                                            maxAngle=65,
                                            angularResolution=55 / 255,
                                            space='theta',
                                            angleMeasure='degrees')

    sample = labels_arr[i]
    r_i = sample[0] * 1.333
    d = sample[1] * 660 / (np.pi * 1.333)
    _, _, _, ind_mie = ps.ScatteringFunction(m=r_i,
                                             wavelength=660,
                                             diameter=d,
                                             nMedium=1.333,
                                             minAngle=10,
                                             maxAngle=65,
                                             angularResolution=55 / 255,
                                             space='theta',
                                             angleMeasure='degrees')

    r = 0
    for i in range(256):
        r += ((ind_nn[i] - ind_mie[i]) ** 2) / 256

    g += r / len(prediction_arr)
g

mse_arr = [11981.002749272884, 12097.661634006527, 12562.540988764631, 13758.843106662813, 32074.865911427787,
           47225.16036969636, 116956.79399418899, 505141.3446767168, 2329560.6060939603]
snr = [10, 8, 5, 3, 1, 0.8, 0.5, 0.3, 0.1]

snr[-7:]

plt.plot(snr, mse_arr, linewidth=1, marker='o')
plt.yscale('log')
plt.xlabel('SNR')
plt.ylabel('MSE')

model = Modified_seven_ConvNet().to(device)
PATH = "/content/drive/MyDrive/Scattering_Group/PyMieScatt/model_Modified_seven_ConvNet_loss_mse__data_100k_scheduler_noise.pt"
optimizer = torch.optim.Adam(model.parameters())

checkpoint = torch.load(PATH, map_location=torch.device('cpu'))
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

epochs = checkpoint['epoch']
loss = checkpoint['loss']
loss_history = checkpoint['loss_history']
train_history = checkpoint['train_history']
val_history = checkpoint['val_history']

epochs = np.linspace(0, len(loss_history), len(loss_history))

prediction_arr, labels_arr = test(metrics=metrics_mape)  # history[3], history[4] #
graphs(loss_history, train_history, val_history, prediction_arr, labels_arr, epochs)

m_true = labels_arr[:, 0]
m_pred = prediction_arr[:, 0]
m_err = abs(m_true[:] - m_pred[:])
plt.hist(m_err, bins=50)

length = len(m_err)
text_1 = "%s" % float('%.2g' % np.median(m_err[(length - 11):(length - 1)]))
print('Абсолютная ошибка m = ' + text_1)
plt.xlim(0, 0.03)
plt.xlabel('Абсолютная ошибка m')
plt.ylabel('Число индикатрис')

x_true = labels_arr[:, 1]
x_pred = prediction_arr[:, 1]
x_err = abs(x_true - x_pred) / x_true * 100
plt.hist(x_err, bins=50)

length = len(x_err)
text_1 = "%s" % float('%.2g' % np.median(x_err[(length - 11):(length - 1)]))  # '%s' % float('%.1g' % 0.012)
print('mape_x = ' + text_1 + '%')
plt.xlim(0, 5)
plt.xlabel('Относительная ошибка x, %')
plt.ylabel('Число индикатрис')

"""Тест кластера

"""

model = Modified_seven_ConvNet().to(device)
# batch0.05k_traindata100k_Modified_seven_ConvNet_nosc
PATH = '/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/batch0.05k_traindata80k_Modified_seven_ConvNet_nosc_restart1.pt'
optimizer = torch.optim.Adam(model.parameters())

checkpoint = torch.load(PATH, map_location=torch.device('cpu'))  # , map_location=torch.device('cpu'))
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

epochs = checkpoint['epoch']
loss = checkpoint['loss']
loss_history = checkpoint['loss_history']
train_history = checkpoint['train_history']
# val_history = checkpoint['val_history']
epochs = np.linspace(0, len(loss_history), len(loss_history))
prediction_arr, labels_arr = test(metrics=metrics_mape)

graphs(loss_history, train_history, prediction_arr, labels_arr, epochs)

model = Modified_seven_ConvNet().to(device)
'/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/LeakyReLU_batch10_traindata100k_sevenconvnet.pt'
'/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/LeakyReLU_batch0.1k_traindata100k_sevenconvnet.pt'
'/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/LeakyReLU_batch10k_traindata100k_sevenconvnet.pt'
'/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/LeakyReLU_batch1k_traindata100k_sevenconvnet.pt'
PATH3 = '/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/batch0.075k_traindata100k_sevenconvnet.pt'
PATH2 = '/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/batch0.05k_traindata100k_sevenconvnet.pt'
PATH1 = "/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/batch10_traindata100k_sevenconvnet.pt"
PATH4 = '/content/drive/MyDrive/Scattering_Group/PyMieScatt/с кластера/batch0.1k_traindata100k_sevenconvnet.pt'

path_arr = [PATH1, PATH2, PATH3, PATH4]
k = 0
for PATH in path_arr:
    k = k + 1
    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))  # , map_location=torch.device('cpu'))
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    epochs = checkpoint['epoch']
    loss = checkpoint['loss']
    loss_history = checkpoint['loss_history']
    train_history = checkpoint['train_history']

    epochs = np.linspace(0, len(loss_history), len(loss_history))

    plt.plot(epochs, train_history, label=k)
    plt.yscale('log')
    plt.legend()


class Modified_five_ConvNet(nn.Module):
    def __init__(self):
        super(Modified_five_ConvNet, self).__init__()
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=2)
        self.bn2 = nn.BatchNorm1d(num_features=2)
        self.conv2 = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm1d(num_features=4)
        self.conv3 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=7, padding=2)
        self.bn4 = nn.BatchNorm1d(num_features=8)
        self.conv4 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=11, padding=2)
        self.bn5 = nn.BatchNorm1d(num_features=16)
        self.conv5 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=13, padding=2)
        self.bn6 = nn.BatchNorm1d(num_features=32)

        self.fc1 = nn.Linear(7744, 180)  #
        self.fc2 = nn.Linear(180, 2)

    def forward(self, x):
        x = self.bn1(x)
        x = F.relu(self.conv1(x))
        x = self.bn2(x)
        x = F.relu(self.conv2(x))
        x = self.bn3(x)
        x = F.relu(self.conv3(x))
        x = self.bn4(x)
        x = F.relu(self.conv4(x))
        x = self.bn5(x)
        x = F.relu(self.conv5(x))
        x = self.bn6(x)

        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return x


class SimpleConvNet(nn.Module):
    def __init__(self):
        super(SimpleConvNet, self).__init__()
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=2)
        self.bn2 = nn.BatchNorm1d(num_features=2)
        self.conv2 = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm1d(num_features=4)
        self.conv3 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=7, padding=2)
        self.bn4 = nn.BatchNorm1d(num_features=8)
        self.fc1 = nn.Linear(256 * 8, 180)  #
        self.fc2 = nn.Linear(180, 2)

    def forward(self, x):
        x = self.bn1(x)
        x = F.relu(self.conv1(x))
        x = self.bn2(x)
        x = F.relu(self.conv2(x))
        x = self.bn3(x)
        x = F.relu(self.conv3(x))
        x = self.bn4(x)
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return x


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=1)
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=4, stride=2)
        self.bn2 = nn.BatchNorm1d(num_features=1)
        self.conv3 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=8, stride=4)
        self.bn3 = nn.BatchNorm1d(num_features=1)
        self.conv4 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=16, stride=8)
        self.bn4 = nn.BatchNorm1d(num_features=1)
        self.conv5 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=32, stride=16)
        self.bn5 = nn.BatchNorm1d(num_features=1)
        self.conv6 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=64, stride=32)
        self.bn6 = nn.BatchNorm1d(num_features=1)

        self.fc1 = nn.Linear(498, 498)
        self.fc2 = nn.Linear(498, 2)

    def forward(self, x):
        conv1 = self.conv1(x)
        bn1 = F.relu(self.bn1(conv1))
        conv2 = self.conv2(x)
        bn2 = F.relu(self.bn2(conv2))
        conv3 = self.conv3(x)
        bn3 = F.relu(self.bn3(conv3))
        conv4 = self.conv4(x)
        bn4 = F.relu(self.bn4(conv4))
        conv5 = self.conv5(x)
        bn5 = F.relu(self.bn5(conv5))
        conv6 = self.conv6(x)
        bn6 = F.relu(self.bn6(conv6))

        mrg = torch.cat([bn1, bn2, bn3, bn4, bn5, bn6], dim=2)
        # drop = nn.Dropout(0.1)(mrg)
        flt = torch.flatten(mrg, 1)

        fc1 = F.relu(self.fc1(flt))
        fc2 = (self.fc2(fc1))

        return fc2


class Net2(nn.Module):
    def __init__(self):
        super(Net2, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=1, stride=1)
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2)
        self.bn2 = nn.BatchNorm1d(num_features=1)
        self.conv3 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=5, stride=4)
        self.bn3 = nn.BatchNorm1d(num_features=1)
        self.conv4 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=7, stride=8)
        self.bn4 = nn.BatchNorm1d(num_features=1)
        self.conv5 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=11, stride=16)
        self.bn5 = nn.BatchNorm1d(num_features=1)
        self.conv6 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=13, stride=32)
        self.bn6 = nn.BatchNorm1d(num_features=1)

        self.fc1 = nn.Linear(502, 502)
        self.fc2 = nn.Linear(502, 2)

    def forward(self, x):
        conv1 = self.conv1(x)
        bn1 = F.relu(self.bn1(conv1))
        conv2 = self.conv2(x)
        bn2 = F.relu(self.bn2(conv2))
        conv3 = self.conv3(x)
        bn3 = F.relu(self.bn3(conv3))
        conv4 = self.conv4(x)
        bn4 = F.relu(self.bn4(conv4))
        conv5 = self.conv5(x)
        bn5 = F.relu(self.bn5(conv5))
        conv6 = self.conv6(x)
        bn6 = F.relu(self.bn6(conv6))

        mrg = torch.cat([bn1, bn2, bn3, bn4, bn5, bn6], dim=2)
        # drop = nn.Dropout(0.1)(mrg)
        flt = torch.flatten(mrg, 1)

        fc1 = F.relu(self.fc1(flt))
        fc2 = (self.fc2(fc1))

        return fc2


# well-known resnet
def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)


class BasicBlock3x3(nn.Module):
    expansion = 1

    def __init__(self, inplanes3, planes, stride=1, downsample=None):
        super(BasicBlock3x3, self).__init__()
        self.conv1 = conv3x3(inplanes3, planes, stride)
        self.bn1 = nn.BatchNorm1d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm1d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet_1(nn.Module):

    def __init__(self, input_channel, layers=[1, 1, 1, 1], num_classes=10):

        self.inplanes3 = 32

        super(ResNet_1, self).__init__()

        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1)

        self.layer3x3_1 = self._make_layer3(BasicBlock3x3, 64, layers[0], stride=2)
        self.layer3x3_2 = self._make_layer3(BasicBlock3x3, 128, layers[1], stride=2)
        self.layer3x3_3 = self._make_layer3(BasicBlock3x3, 256, layers[2], stride=2)
        self.layer3x3_4 = self._make_layer3(BasicBlock3x3, 512, layers[3], stride=2)

        self.conv2 = nn.Conv1d(512, 32, kernel_size=3, stride=2)
        self.conv3 = nn.Conv1d(32, 1, kernel_size=3, stride=2)

        self.linear = nn.Linear(100, 2)

    def _make_layer3(self, block, planes, blocks, stride=2):
        downsample = None
        if stride != 1 or self.inplanes3 != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv1d(self.inplanes3, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm1d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes3, planes, stride, downsample))
        self.inplanes3 = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes3, planes))

        return nn.Sequential(*layers)

    def forward(self, x):

        x = self.conv1(x)

        x = self.layer3x3_1(x)
        x = self.layer3x3_2(x)
        x = self.layer3x3_3(x)
        x = self.layer3x3_4(x)

        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        x = x.squeeze()
        x = F.relu(self.linear(x))

        return x


class Old_Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=1)
        self.bn1 = nn.BatchNorm1d(num_features=1)
        self.conv2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=4, stride=2)
        self.bn2 = nn.BatchNorm1d(num_features=1)
        self.conv3 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=8, stride=4)
        self.bn3 = nn.BatchNorm1d(num_features=1)
        self.conv4 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=16, stride=8)
        self.bn4 = nn.BatchNorm1d(num_features=1)
        self.conv5 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=32, stride=16)
        self.bn5 = nn.BatchNorm1d(num_features=1)
        self.conv6 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=64, stride=32)
        self.bn6 = nn.BatchNorm1d(num_features=1)

        self.fc1 = nn.Linear(246, 246)
        self.fc2 = nn.Linear(246, 2)

    def forward(self, x):
        bn1 = self.bn1(x)
        conv1 = F.relu(self.conv1(bn1))
        bn2 = self.bn2(x)
        conv2 = F.relu(self.conv2(bn2))
        bn3 = self.bn3(x)
        conv3 = F.relu(self.conv3(bn3))
        bn4 = self.bn4(x)
        conv4 = F.relu(self.conv4(bn4))
        bn5 = self.bn5(x)
        conv5 = F.relu(self.conv5(bn5))
        bn6 = self.bn6(x)
        conv6 = F.relu(self.conv6(bn6))

        mrg = torch.cat([conv1, conv2, conv3, conv4, conv5, conv6], dim=2)
        # drop = nn.Dropout(0.1)(mrg)
        flt = torch.flatten(mrg, 1)

        fc1 = F.relu(self.fc1(flt))
        fc2 = (self.fc2(fc1))

        return fc2


# working example
def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)


class BasicBlock3x3(nn.Module):
    expansion = 1

    def __init__(self, inplanes3, planes, stride=1, downsample=None):
        super(BasicBlock3x3, self).__init__()
        self.conv1 = conv3x3(inplanes3, planes, stride)
        self.bn1 = nn.BatchNorm1d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm1d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet_1(nn.Module):

    def __init__(self, input_channel, layers=[1, 1, 1, 1], num_classes=10):

        self.inplanes3 = 64

        super(ResNet_1, self).__init__()

        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=1)
        self.bn = nn.BatchNorm1d(64)
        self.relu = nn.ReLU(inplace=True)

        self.layer3x3_1 = self._make_layer3(BasicBlock3x3, 64, layers[0], stride=2)
        self.layer3x3_2 = self._make_layer3(BasicBlock3x3, 128, layers[1], stride=2)
        self.layer3x3_3 = self._make_layer3(BasicBlock3x3, 256, layers[2], stride=2)
        # self.layer3x3_4 = self._make_layer3(BasicBlock3x3, 512, layers[3], stride=2)

        self.conv2 = nn.Conv1d(256, 32, kernel_size=3, stride=2)
        self.conv3 = nn.Conv1d(32, 1, kernel_size=3, stride=2)

        self.linear = nn.Linear(3, 2)

    def _make_layer3(self, block, planes, blocks, stride=2):
        downsample = None
        if stride != 1 or self.inplanes3 != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv1d(self.inplanes3, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm1d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes3, planes, stride, downsample))
        self.inplanes3 = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes3, planes))

        return nn.Sequential(*layers)

    def forward(self, x):

        x = self.conv1(x)
        x = self.bn(x)
        x = self.relu(x)

        x = self.layer3x3_1(x)
        x = self.layer3x3_2(x)
        x = self.layer3x3_3(x)
        # x = self.layer3x3_4(x)

        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        x = x.squeeze()
        x = F.relu(self.linear(x))

        return x

